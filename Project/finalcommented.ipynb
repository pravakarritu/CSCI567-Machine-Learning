{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Importing the libraries"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:47.318017Z","iopub.status.busy":"2022-12-12T23:55:47.317029Z","iopub.status.idle":"2022-12-12T23:55:47.613885Z","shell.execute_reply":"2022-12-12T23:55:47.612502Z","shell.execute_reply.started":"2022-12-12T23:55:47.317933Z"},"papermill":{"duration":1.772534,"end_time":"2022-05-27T10:44:40.791546","exception":false,"start_time":"2022-05-27T10:44:39.019012","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    Import the necessary libraries\n","    The numpy and pandas libraries are commonly used for data manipulation and analysis, \n","    matplotlib and seaborn are used for data visualization, \n","    and sklearn contains a variety of machine learning algorithms. \n","    train_test_split and cross_val_score are used for model evaluation, \n","    and LinearRegression is a machine learning algorithm for regression tasks.\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","        \n","from scipy.stats import skew,norm,zscore\n","from scipy.signal import periodogram\n","\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n","\n","from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, GridSearchCV, cross_validate\n","from sklearn.metrics import mean_squared_error, make_scorer, mean_squared_log_error, mean_absolute_error, mean_absolute_percentage_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor"]},{"cell_type":"markdown","metadata":{},"source":["# Importing the dataset"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:48.020436Z","iopub.status.busy":"2022-12-12T23:55:48.018568Z","iopub.status.idle":"2022-12-12T23:55:53.120471Z","shell.execute_reply":"2022-12-12T23:55:53.119079Z","shell.execute_reply.started":"2022-12-12T23:55:48.020309Z"},"papermill":{"duration":3.889064,"end_time":"2022-05-27T10:44:44.758882","exception":false,"start_time":"2022-05-27T10:44:40.869818","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code reads in several CSV files containing data for a machine learning project on sales forecasting using time series. \n","    The holidays_events, oil, stores, and transactions files are read into orig_holidays_events, orig_oil, orig_stores, \n","    and orig_transactions DataFrames, respectively. \n","    \n","    The test and train files are also read into DataFrames named orig_test and orig_train, respectively.\n","    \n","    The parse_dates option is used when reading the CSV files to specify that the date column should be parsed as a date instead of a string. \n","    This allows the date values to be treated as dates in the DataFrames, which makes it easier to work with them and perform time series analysis.\n","\"\"\"\n","\n","# Read csv files and parse dates for date columns\n","orig_holidays_events = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\", parse_dates=['date'])\n","orig_oil = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\", parse_dates=['date'])\n","orig_stores = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\")\n","orig_transactions = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\", parse_dates=['date'])\n","\n","# Read test and train csv files and parse dates for date columns\n","orig_test = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\", parse_dates=['date'])\n","orig_train = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\", parse_dates=['date'])"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.022931,"end_time":"2022-05-27T10:44:45.2275","exception":false,"start_time":"2022-05-27T10:44:45.204569","status":"completed"},"tags":[]},"source":["# Setting dates"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:53.123770Z","iopub.status.busy":"2022-12-12T23:55:53.123178Z","iopub.status.idle":"2022-12-12T23:55:53.132075Z","shell.execute_reply":"2022-12-12T23:55:53.130135Z","shell.execute_reply.started":"2022-12-12T23:55:53.123718Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code is used to set the dates for the train, test, and forecast periods.\n","    date_start_train and date_end_train specify the start and end dates for the training data, \n","    and date_start_test and date_end_test specify the start and end dates for the test data. \n","    The date_start_fore date may be used to specify the start of a forecasting period.\n","\"\"\"\n","\n","date = {\n","    'date_start_train': '2013-01-01',\n","    'date_end_train': '2017-08-15',\n","    'date_start_test': '2017-08-16',\n","    'date_end_test': '2017-08-31',\n","    'date_start_fore': '2016-06-01'\n","}"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:53.134085Z","iopub.status.busy":"2022-12-12T23:55:53.133584Z","iopub.status.idle":"2022-12-12T23:55:53.164196Z","shell.execute_reply":"2022-12-12T23:55:53.162617Z","shell.execute_reply.started":"2022-12-12T23:55:53.134047Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code computes the number of days between two dates for the training and test data. \n","    It does this by using the pd.Timestamp class from the pandas library to convert the dates stored in the date dictionary to Timestamp objects. \n","    Then, it subtracts the two Timestamp objects to compute the number of days between them.\n","\n","    The diff_train variable stores the number of days between the date_end_train and date_start_fore dates, \n","    and the diff_test variable stores the number of days between the date_end_test and date_start_fore dates.\n","\"\"\"\n","\n","diff_train = (pd.Timestamp(date['date_end_train']) - pd.Timestamp(date['date_start_fore'])).days\n","diff_test = (pd.Timestamp(date['date_end_test']) - pd.Timestamp(date['date_start_fore'])).days"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.024242,"end_time":"2022-05-27T10:44:45.330202","exception":false,"start_time":"2022-05-27T10:44:45.30596","status":"completed"},"tags":[]},"source":["# Stores Preprocessing"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:53.169008Z","iopub.status.busy":"2022-12-12T23:55:53.168229Z","iopub.status.idle":"2022-12-12T23:55:53.182591Z","shell.execute_reply":"2022-12-12T23:55:53.180882Z","shell.execute_reply.started":"2022-12-12T23:55:53.168954Z"},"papermill":{"duration":0.034752,"end_time":"2022-05-27T10:44:45.54922","exception":false,"start_time":"2022-05-27T10:44:45.514468","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This function store_func takes a DataFrame as an argument. \n","    The function adds some new features to the DataFrame and then merges it with the orig_train and orig_test DataFrames. \n","    The resulting DataFrame is returned by the function.\n","    \n","    Inside the function, the uniquestore and newstore features are added to the DataFrame by applying anonymous functions (also known as lambda functions) to the city and store_nbr columns. \n","    These features may be useful for the machine learning model because they encode additional information about the stores in the data.\n","    \n","    Next, the function uses the concat method to combine the orig_train and orig_test DataFrames into a single DataFrame. \n","    Then, it uses the merge method to merge this DataFrame with the DataFrame passed to the store_func function, based on the store_nbr column. \n","    Finally, the function renames the type column to store and returns the resulting DataFrame.\n","\"\"\"\n","\n","def store_func (orig_df):\n","    \n","    # Create a copy of the original dataframe\n","    df = orig_df.copy()\n","    \n","    # Add features to the original stores dataframe\n","    df['uniquestore'] = df.city.apply(lambda x: 0 if x in ['Quito', 'Guayaquil', 'Santo Domingo', 'Cuenca', 'Manta', 'Machala', 'Latacunga', 'Ambato'] else 1)\n","    df['newstore'] = df.store_nbr.apply(lambda x: 1 if x in [19, 20, 21, 28, 35, 41, 51, 52] else 0)\n","        \n","    # Merge the original stores dataframe, original test dataframe and original train dataframe\n","    df = pd.concat([orig_train, orig_test], axis=0).merge(df, on=['store_nbr'], how='left')\n","    df = df.rename(columns={'type' : 'store'}) \n","\n","    return df"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:53.186039Z","iopub.status.busy":"2022-12-12T23:55:53.185369Z","iopub.status.idle":"2022-12-12T23:55:55.979575Z","shell.execute_reply":"2022-12-12T23:55:55.978131Z","shell.execute_reply.started":"2022-12-12T23:55:53.185983Z"},"papermill":{"duration":1.129152,"end_time":"2022-05-27T10:44:46.701785","exception":false,"start_time":"2022-05-27T10:44:45.572633","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code calls the store_func function and passes the orig_stores DataFrame as an argument. \n","    The function returns a new DataFrame that includes the original data from orig_stores, \n","    as well as some additional features and data from the orig_train and orig_test DataFrames. \n","    \n","    This new DataFrame is stored in the final_df variable.\n","\"\"\"\n","\n","final_df = store_func(orig_stores)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.023523,"end_time":"2022-05-27T10:44:46.748707","exception":false,"start_time":"2022-05-27T10:44:46.725184","status":"completed"},"tags":[]},"source":["# Events Preprocessing"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:55.982962Z","iopub.status.busy":"2022-12-12T23:55:55.981840Z","iopub.status.idle":"2022-12-12T23:55:56.004489Z","shell.execute_reply":"2022-12-12T23:55:56.003045Z","shell.execute_reply.started":"2022-12-12T23:55:55.982889Z"},"papermill":{"duration":0.050665,"end_time":"2022-05-27T10:44:47.375169","exception":false,"start_time":"2022-05-27T10:44:47.324504","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code defines a function holiday_func that takes a DataFrame as an argument. \n","    The function processes the data in the DataFrame and then merges it with the final_df DataFrame, which was created by the store_func function earlier. \n","    The resulting DataFrame is returned by the function.\n","\n","    Inside the function, the first step is to create a copy of the input DataFrame and store it in a new variable named df. \n","    Then, the function makes several modifications to the data in df. \n","    It removes duplicates and adds new features, such as event_type and isevent.\n","    \n","    Next, the function uses the merge method to merge the final_df DataFrame with df. \n","    This combines the data from both DataFrames into a single DataFrame that can be used to train a machine learning model.\n","    \n","    Finally, the function applies a series of transformations to the data in the resulting DataFrame, such as adding Easter and closure days. \n","    The resulting DataFrame is returned by the function.\n","\"\"\"\n","\n","def holiday_func (orig_df):\n","    \n","    df = orig_df.copy()\n","    \n","    # Non-transferred events\n","    df.loc[297, 'transferred'] = df.loc[297, 'transferred'] = False\n","    df = df.query(\"transferred!=True\")\n","    \n","    # Removing duplicates\n","    df = df.drop(index=orig_holidays_events[orig_holidays_events[['date', 'locale_name']].duplicated()].index.values)\n","\n","    # Adding event type\n","    df.loc[df.type=='Event', 'type'] = df.description.apply(lambda x: x[0:7])\n","     \n","    # Merging orig_holidays_events and final_df\n","    nat_df = df.query(\"locale=='National'\")\n","    loc_df = df.query(\"locale=='Local'\")\n","    reg_df = df.query(\"locale=='Regional'\")\n","    \n","    df = final_df.merge(nat_df, left_on=['date'], right_on=['date'], how='left')\n","    df = df.merge(loc_df, left_on=['date', 'city'], right_on=['date', 'locale_name'], how='left')\n","    df = df.merge(reg_df, left_on=['date', 'state'], right_on=['date', 'locale_name'], how='left')\n","   \n","    # Adding New Year\n","    df['firstday'] = df.description_x.apply(lambda x: 1 if x=='Primer dia del ano' else 0)\n","\n","    # Matching event and store\n","    df = df.drop(columns=['locale_x', 'locale_name_x', 'description_x', 'transferred_x',\n","                          'locale_y', 'locale_name_y', 'description_y', 'transferred_y',\n","                          'locale', 'locale_name', 'description', 'transferred'])\n","    df.loc[~df.type_x.isnull(), 'event_type'] = df.type_x.apply(lambda x: x)\n","    df.loc[~df.type_y.isnull(), 'event_type'] = df.type_y.apply(lambda x: x)\n","    df.loc[~df.type.isnull(), 'event_type'] = df.type.apply(lambda x: x)\n","    df.loc[df.event_type.isnull(), 'event_type'] = df.event_type.apply(lambda x: 'norm')\n","    df = df.drop(columns=['type_x', 'type_y', 'type'])\n","\n","    df['isevent'] = df.event_type.apply(lambda x: 'y' if x!='norm' else 'n')\n","\n","    # Adding Easter\n","    df.loc[df.date.isin(['2017-04-16', '2016-03-27', '2015-04-05', '2014-04-20', '2013-03-31']), 'isevent'] = df.isevent.apply(lambda x: 'y')\n","    df.loc[df.date.isin(['2017-04-16', '2016-03-27', '2015-04-05', '2014-04-20', '2013-03-31']), 'event_type'] = df.event_type.apply(lambda x: 'Holiday')\n","\n","    # Adding closure days\n","    df['isclosed'] = df.groupby(by=['date', 'store_nbr'])['sales'].transform(lambda x: 1 if x.sum()==0 else 0)    \n","    df.loc[(df.date.dt.year==2017) & (df.date.dt.month==8) & (df.date.dt.day>=16) , 'isclosed'] = df.isclosed.apply(lambda x: 0)    \n","    df.loc[df.date.isin(['2017-01-01']), 'isevent'] = df.isevent.apply(lambda x: 'n')\n","  \n","    return df"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:55:56.007174Z","iopub.status.busy":"2022-12-12T23:55:56.006390Z","iopub.status.idle":"2022-12-12T23:56:41.702330Z","shell.execute_reply":"2022-12-12T23:56:41.701104Z","shell.execute_reply.started":"2022-12-12T23:55:56.007133Z"},"papermill":{"duration":43.079371,"end_time":"2022-05-27T10:45:30.480184","exception":false,"start_time":"2022-05-27T10:44:47.400813","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code calls the holiday_func function that was defined earlier and passes the orig_holidays_events DataFrame as an argument. \n","    The function returns a new DataFrame that includes the original data from orig_holidays_events, as well as some additional features and data from the final_df DataFrame. \n","    This new DataFrame is stored in the final_df variable.\n","    \n","    holiday_func function is being used to further preprocess the data in preparation for training a machine learning model. \n","    The new features and data added to the DataFrame by the function are useful for the model and improve its performance. \n","    This code overwrites the previous value of final_df, which was created by the store_func function.\n","\"\"\"\n","\n","final_df = holiday_func(orig_holidays_events)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.023088,"end_time":"2022-05-27T10:45:30.527013","exception":false,"start_time":"2022-05-27T10:45:30.503925","status":"completed"},"tags":[]},"source":["# Oil Preprocessing"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:56:41.705113Z","iopub.status.busy":"2022-12-12T23:56:41.704536Z","iopub.status.idle":"2022-12-12T23:56:41.733568Z","shell.execute_reply":"2022-12-12T23:56:41.732270Z","shell.execute_reply.started":"2022-12-12T23:56:41.705063Z"},"papermill":{"duration":0.030599,"end_time":"2022-05-27T10:45:30.634306","exception":false,"start_time":"2022-05-27T10:45:30.603707","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["dcoilwtico    529\n","dtype: int64"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","    This code reads in the oil.csv file, which contains data about the price of oil. \n","    It uses the pd.read_csv function from the pandas library to read the data into a DataFrame, \n","    and then it uses the to_datetime method to convert the date column from a string to a datetime format.\n","    \n","    Next, the code sets the date column as the index of the DataFrame using the set_index method. \n","    This allows the data to be easily accessed and manipulated using the dates as keys.\n","    \n","    The code then uses the resample method to resample the data to a daily frequency and compute the mean price of oil for each day. \n","    Finally, it uses the isnull and sum methods to compute the number of missing values in the DataFrame. \n","    \n","    This information may be useful for understanding the quality of the data and determining whether any further preprocessing is needed.\n","\"\"\"\n","\n","# read in the oil data\n","oil = pd.read_csv('oil.csv')\n","\n","# change the date column to a datetime format\n","oil['date'] = pd.to_datetime(oil['date'])\n","\n","# set the date as the index\n","oil = oil.set_index('date')\n","\n","# resample the data to daily and get the mean\n","oil = oil.resample(\"D\").mean()\n","\n","# get the number of null values\n","oil.isnull().sum()\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:56:41.735443Z","iopub.status.busy":"2022-12-12T23:56:41.734989Z","iopub.status.idle":"2022-12-12T23:56:41.744729Z","shell.execute_reply":"2022-12-12T23:56:41.743384Z","shell.execute_reply.started":"2022-12-12T23:56:41.735411Z"},"papermill":{"duration":0.032864,"end_time":"2022-05-27T10:45:30.691695","exception":false,"start_time":"2022-05-27T10:45:30.658831","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code defines a function oil_func that takes a DataFrame as an argument. \n","    The function processes the data in the DataFrame \n","    and then merges it with the final_df DataFrame, which was created by the holiday_func function earlier. \n","    The resulting DataFrame is returned by the function.\n","    \n","    Inside the function, the first step is to create a copy of the input DataFrame and store it in a new variable named df. \n","    Then, the function adds missing values to df by using the set_index method to set the date column as the index, \n","    the resample method to resample the data to a daily frequency, the interpolate method to fill in missing values, \n","    and the reset_index method to restore the original index of the DataFrame.\n","    \n","    Next, the function adds several lag features to the data by using a for loop to shift the dcoilwtico column \n","    by a specified number of days and create a new column for each shift. \n","    The function also adds moving average features by using the rolling method to compute \n","    the average value of the dcoilwtico column over a specified number of days.\n","    \n","    Finally, the function uses the merge method to merge the final_df DataFrame with df, \n","    and then it returns the resulting DataFrame. This DataFrame includes the original data from both final_df and df, \n","    as well as the new features that were added by the oil_func function.\n","\"\"\"\n","\n","def oil_func (orig_df):\n","    \n","    df = orig_df.copy()\n","    \n","    # Adding missing values\n","    df = df.set_index('date').resample(\"D\").mean().interpolate(limit_direction='backward').reset_index()\n","    \n","    # Adding lag features\n","    for i in [1, 2, 3, 4, 5, 6, 7, 10, 14, 21, 30, 60, 90]:\n","        df['lagoil_' + str(i) + '_dcoilwtico'] = df['dcoilwtico'].shift(i)\n","    \n","    # Adding moving average features\n","    df['oil_week_avg'] = df['dcoilwtico'].rolling(7).mean()\n","    df['oil_2weeks_avg'] = df['dcoilwtico'].rolling(14).mean()\n","    df['oil_month_avg'] = df['dcoilwtico'].rolling(30).mean()\n","\n","    # drop rows with null values\n","    df.dropna(inplace = True)\n","    \n","    # Merging orig_oil and final_df\n","    df = final_df.merge(df, on=['date'], how='left')\n","    \n","    return df"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:56:41.749143Z","iopub.status.busy":"2022-12-12T23:56:41.748662Z","iopub.status.idle":"2022-12-12T23:56:44.566441Z","shell.execute_reply":"2022-12-12T23:56:44.564804Z","shell.execute_reply.started":"2022-12-12T23:56:41.749107Z"},"papermill":{"duration":1.762009,"end_time":"2022-05-27T10:45:32.479379","exception":false,"start_time":"2022-05-27T10:45:30.71737","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code calls the oil_func function that was defined earlier and passes the orig_oil DataFrame as an argument. \n","    The function returns a new DataFrame that includes the original data from orig_oil, \n","    as well as some additional features and data from the final_df DataFrame. \n","    This new DataFrame is stored in the final_df variable.\n","    \n","    The oil_func function is being used to further preprocess the data in preparation for training a machine learning model. \n","    The new features and data added to the DataFrame by the function are useful for the model and improve its performance. \n","    \n","    This code overwrites the previous value of final_df, which was created by the holiday_func function.\n","\"\"\"\n","\n","# store the dataframe in the final_df\n","final_df = oil_func(orig_oil)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.016602,"end_time":"2022-05-27T10:45:32.513307","exception":false,"start_time":"2022-05-27T10:45:32.496705","status":"completed"},"tags":[]},"source":["# Transactions Preprocessing"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:56:44.568838Z","iopub.status.busy":"2022-12-12T23:56:44.568334Z","iopub.status.idle":"2022-12-12T23:56:44.582991Z","shell.execute_reply":"2022-12-12T23:56:44.581425Z","shell.execute_reply.started":"2022-12-12T23:56:44.568801Z"},"papermill":{"duration":0.030525,"end_time":"2022-05-27T10:45:32.637102","exception":false,"start_time":"2022-05-27T10:45:32.606577","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code defines a function transactions_func that takes a DataFrame as an argument. \n","    The function processes the data in the DataFrame and then merges it with the final_df DataFrame, \n","    which was created by the oil_func function earlier. The resulting DataFrame is returned by the function.\n","    \n","    Inside the function, the first step is to create a copy of the input DataFrame and store it in a new variable named df. \n","    Then, the function uses the merge method to merge df with the final_df DataFrame, \n","    which includes the data from orig_oil and orig_holidays_events.\n","    \n","    Next, the function fills in missing values in the transactions column of df. \n","    It does this by using the loc method to identify rows where the transactions column is null and the isclosed column is 1, \n","    and then it sets the transactions value to 0 for these rows.\n","    \n","    The function then groups the data by store_nbr and date, computes the average transactions for each group, \n","    and adds some lag features to the data. It then uses the drop method to remove the transactions column, \n","    and then it merges the resulting DataFrame with the original df. \n","    \n","    Finally, the function fills in any remaining missing values in the transactions column \n","    by using the average transactions computed earlier, and then it returns the resulting DataFrame.\n","\"\"\"\n","\n","def transactions_func (orig_df):\n","    \n","    df = orig_df.copy()\n","    \n","    # Merging orig_transactions and final_df\n","    df = final_df.merge(df, on=['date', 'store_nbr'], how='left')\n","    \n","    # Filling missing values\n","    df.loc[(df.transactions.isnull()) & (df.isclosed==1), 'transactions'] = df.transactions.apply(lambda x: 0)\n","    group_df = df.groupby(by=['store_nbr', 'date']).transactions.first().reset_index()\n","    group_df['avg_tra'] = group_df.transactions.rolling(15, min_periods=10).mean()\n","    group_df['16_tra'] = group_df.transactions.shift(16)\n","    group_df['21_tra'] = group_df.transactions.shift(21)\n","    group_df['30_tra'] = group_df.transactions.shift(30)\n","    group_df['60_tra'] = group_df.transactions.shift(60)\n","    \n","    group_df.drop(columns='transactions', inplace=True)\n","    \n","    df = df.merge(group_df, on=['date', 'store_nbr'], how='left')\n","    df.loc[(df.transactions.isnull()) & (df.isclosed==0), 'transactions'] = df.avg_tra\n","    \n","    df.drop(columns='avg_tra', inplace=True)\n","    df.loc[(df.date.dt.year==2017) & (df.date.dt.month==8) & (df.date.dt.day>=16) , 'transactions'] = df.transactions.apply(lambda x: None)    \n","\n","    df['tot_store_day_onprom'] = df.groupby(by=['date', 'store_nbr']).onpromotion.transform(lambda x: x.sum())\n","\n","    return df"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:56:44.585176Z","iopub.status.busy":"2022-12-12T23:56:44.584732Z","iopub.status.idle":"2022-12-12T23:57:17.882175Z","shell.execute_reply":"2022-12-12T23:57:17.880911Z","shell.execute_reply.started":"2022-12-12T23:56:44.585141Z"},"papermill":{"duration":31.0633,"end_time":"2022-05-27T10:46:03.714874","exception":false,"start_time":"2022-05-27T10:45:32.651574","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code calls the transactions_func function that was defined earlier and passes the orig_transactions DataFrame as an argument. \n","    The function returns a new DataFrame that includes the original data from orig_transactions, \n","    as well as some additional features and data from the final_df DataFrame. \n","    This new DataFrame is stored in the final_df variable.\n","\n","    The transactions_func function is being used to further preprocess the data in preparation for training a machine learning model. \n","    The new features and data added to the DataFrame by the function are useful for the model and improve its performance. \n","    This code overwrites the previous value of final_df, which was created by the oil_func function\n","\"\"\"\n","\n","# store the dataframe in the final_df\n","final_df = transactions_func(orig_transactions)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:17.884618Z","iopub.status.busy":"2022-12-12T23:57:17.884020Z","iopub.status.idle":"2022-12-12T23:57:17.908377Z","shell.execute_reply":"2022-12-12T23:57:17.906548Z","shell.execute_reply.started":"2022-12-12T23:57:17.884563Z"},"papermill":{"duration":0.037833,"end_time":"2022-05-27T10:46:03.826827","exception":false,"start_time":"2022-05-27T10:46:03.788994","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code uses the del keyword to delete several variables that were defined earlier in the code. \n","    These variables include the original data that was read in from CSV files at the beginning of the script, \n","    as well as some intermediate DataFrames that were created by the various preprocessing functions.\n","    \n","    Deleting these variables frees up memory and prevents them from cluttering the workspace. \n","    Since the data in these variables has been processed and stored in the final final_df DataFrame, \n","    it is no longer necessary to keep the original variables around.\n","\"\"\"\n","\n","del orig_train\n","del orig_test\n","del orig_stores\n","del orig_holidays_events\n","del orig_oil\n","del orig_transactions"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.020919,"end_time":"2022-05-27T10:46:03.869075","exception":false,"start_time":"2022-05-27T10:46:03.848156","status":"completed"},"tags":[]},"source":["# Final dataframe"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:17.911168Z","iopub.status.busy":"2022-12-12T23:57:17.910667Z","iopub.status.idle":"2022-12-12T23:57:19.951277Z","shell.execute_reply":"2022-12-12T23:57:19.949713Z","shell.execute_reply.started":"2022-12-12T23:57:17.911133Z"},"papermill":{"duration":0.751296,"end_time":"2022-05-27T10:46:04.692391","exception":false,"start_time":"2022-05-27T10:46:03.941095","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code uses the set_index and loc methods to create a new DataFrame that includes only the rows in the final_df DataFrame \n","    that have a date greater than or equal to the date_start_fore value specified in the date dictionary. \n","    The set_index method sets the date column as the index for the DataFrame, \n","    and the loc method is used to filter the rows based on their date values. \n","    The resulting DataFrame is then stored back in the final_df variable.\n","    \n","    This code is further preprocessing the data in preparation for training a machine learning model. \n","    By limiting the data to only those rows with dates greater than or equal to the date_start_fore value, \n","    the code is creating a training set for the model that includes only the most recent data. \n","    This could improve the performance of the model and make it more accurate.\n","\"\"\"\n","\n","final_df = final_df.set_index('date').loc[date['date_start_fore']:,:]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Additional Support functions"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:19.953538Z","iopub.status.busy":"2022-12-12T23:57:19.953067Z","iopub.status.idle":"2022-12-12T23:57:19.961594Z","shell.execute_reply":"2022-12-12T23:57:19.960110Z","shell.execute_reply.started":"2022-12-12T23:57:19.953502Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    This code defines a split_func function that takes four arguments: orig_df, X, y, end_date, and test_size. \n","    orig_df is a DataFrame that contains the original data, \n","    X is a DataFrame that contains the input features for the machine learning model, \n","    y is a DataFrame that contains the target variable for the model, \n","    end_date is a date that specifies the end of the training period, \n","    and test_size is a float that indicates the proportion of the data that should be used for testing.\n","\n","    The function uses the train_test_split function from the sklearn.model_selection module to split the data into training and test sets. \n","    This function takes the X and y DataFrames as arguments and returns four DataFrames: X_train, y_train, X_test, and y_test. \n","    These DataFrames contain the input features and target variable for the training and test sets, respectively. \n","    The split_func function then returns these four DataFrames.\n","    \n","    This function is used to split the data into training and test sets in preparation for training a machine learning model. \n","    The train_test_split function ensures that the data is split into mutually exclusive sets, \n","    so that the training set is used to train the model and the test set is used to evaluate its performance. \n","    \n","    This is an important step in the machine learning process, \n","    as it helps to prevent overfitting and ensures that the model will generalize well to new data.\n","\"\"\"\n","\n","def split_func (orig_df, X, y, end_date, test_size):\n","    \"\"\"\n","    This function splits the original dataframe into a training set and testing set\n","    using a specified test size. The original dataframe is split into X and y dataframes\n","    and the X_train and X_test dataframes are returned along with the y_train and y_test\n","    dataframes.\n","    \n","    Parameters\n","    ----------\n","    orig_df: pandas DataFrame\n","        The original dataframe that will be split into train and test sets.\n","    X: pandas DataFrame\n","        The X portion of the original dataframe.\n","    y: pandas DataFrame\n","        The y portion of the original dataframe.\n","    end_date: datetime\n","        The end date of the train and test split.\n","    test_size: float\n","        The percentage of the original dataframe to be used for testing.\n","    \n","    Returns\n","    -------\n","    X_train: pandas DataFrame\n","        The X portion of the training dataframe.\n","    y_train: pandas DataFrame\n","        The y portion of the training dataframe.\n","    X_test: pandas DataFrame\n","        The X portion of the testing dataframe.\n","    y_test: pandas DataFrame\n","        The y portion of the testing dataframe.\n","    \"\"\"\n","    \n","    # Splitting train and test\n","    idx_train, idx_test = train_test_split(orig_df.index, test_size=test_size, shuffle=False)\n","    X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n","    y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n","    \n","    return X_train, y_train, X_test, y_test"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:19.964487Z","iopub.status.busy":"2022-12-12T23:57:19.963826Z","iopub.status.idle":"2022-12-12T23:57:19.980972Z","shell.execute_reply":"2022-12-12T23:57:19.979478Z","shell.execute_reply.started":"2022-12-12T23:57:19.964444Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    The my_split_func() function takes a dataframe and creates a train set, validation set, and the corresponding target sets. \n","    The train set and validation set are created by filtering the dataframe by the specified start and end dates for each set. \n","    The target sets are created by selecting the 'sales' column from the corresponding sets.\n","    \n","    The start and end dates for the train and validation sets are specified as function parameters. \n","    The function also calculates the number of days from the start of the year 2013 to each date in the original dataframe \n","    and adds this as a new column called 'days_from_2013' to the dataframe. \n","    \n","    This new column is used to filter the dataframe to create the train and validation sets.\n","    After creating the sets, the function drops the 'sales' column from the feature sets and returns all four sets.\n","\n","    train_start_date is the first date in the training set\n","    train_end_date is the last date in the training set\n","    val_start_date is the first date in the validation set\n","    val_end_date is the last date in the validation set\n","\"\"\"\n","\n","def my_split_func(df, train_start_date='2013-01-01', train_end_date='2017-08-30',\n","               val_start_date='2017-09-01', val_end_date='2020-01-01'):\n","    train_start_date = (pd.to_datetime(train_start_date) - pd.to_datetime('2013-01-01')).days\n","    train_end_date = (pd.to_datetime(train_end_date) - pd.to_datetime('2013-01-01')).days\n","    val_start_date = (pd.to_datetime(val_start_date) - pd.to_datetime('2013-01-01')).days\n","    val_end_date = (pd.to_datetime(val_end_date) - pd.to_datetime('2013-01-01')).days\n","    \n","    train = df[(df['days_from_2013'] >= train_start_date) & (df['days_from_2013'] <= train_end_date)]\n","    val = df[(df['days_from_2013'] >= val_start_date) & (df['days_from_2013'] <= val_end_date)]\n","    return [train.drop(columns=['sales']), val.drop(columns=['sales']), train['sales'], val['sales']]"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:19.983433Z","iopub.status.busy":"2022-12-12T23:57:19.982912Z","iopub.status.idle":"2022-12-12T23:57:20.006834Z","shell.execute_reply":"2022-12-12T23:57:20.004906Z","shell.execute_reply.started":"2022-12-12T23:57:19.983393Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    This function takes in a number tp and a series of dates, and returns an array of weights. \n","    The value of the weight depends on the value of tp:\n","    \n","    If tp is 1, the function returns an array of ones with the same length as the dates series.\n","    \n","    If tp is 2, the function returns an array of weights that decrease exponentially as the date gets closer to August 16, 2017. \n","    The rate of the decay is determined by the formula np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 100), \n","    where 400 is a constant that determines the starting weight, '2017-08-16' is the reference date, and dates is the series of dates.\n","    \n","    If tp is 3, the function returns an array of weights that decrease exponentially as the date gets closer to August 16, 2017, \n","    but at a slower rate than for tp = 2. The rate of the decay is determined by the formula \n","    np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 200).\n","    \n","    If tp is 4, the function returns an array of weights that decrease exponentially as the date gets closer to August 16, 2017, \n","    but at an even slower rate than for tp = 3. \n","    The rate of the decay is determined by the formula np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 300).\n","    \n","    If tp is 5, the function returns an array of weights that decrease exponentially as the date gets closer to August 16, 2017, \n","    but at the slowest rate of all the possible values of tp. \n","    The rate of the decay is determined by the formula np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 400).\n","\"\"\"\n","\n","def get_weights_distribution(tp, dates):\n","    if tp == 1:\n","        return np.ones(dates.shape)\n","    if tp == 2:\n","        return np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 100)\n","    if tp == 3:\n","        return np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 200)\n","    if tp == 4:\n","        return np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 300)\n","    if tp == 5:\n","        return np.exp((400 - (pd.to_datetime('2017-08-16') - pd.to_datetime(dates)).days) / 400)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:20.010292Z","iopub.status.busy":"2022-12-12T23:57:20.009577Z","iopub.status.idle":"2022-12-12T23:57:20.031287Z","shell.execute_reply":"2022-12-12T23:57:20.029961Z","shell.execute_reply.started":"2022-12-12T23:57:20.010250Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    The tags_to_dict() function takes no arguments and returns a list of five dictionaries. \n","    The first dictionary maps store item tags to an integer value representing the average percentage of male shoppers that buy the items with that tag. \n","    The second dictionary maps store item tags to an integer value representing the average percentage of shoppers that buy luxury items with that tag. \n","    The third dictionary maps store item tags to an integer value representing the average age of shoppers that buy items with that tag. \n","    The fourth dictionary maps store item tags to an integer value representing the variance in the age of shoppers that buy items with that tag. \n","    The fifth dictionary maps store item tags to a string value representing the type of items with that tag (e.g., food, family, other).\n","\"\"\"\n","\n","def tags_to_dict():\n","    tags = {\n","     'AUTOMOTIVE': [4, 7, 30, 10, 'family'],\n","     'BABY CARE':  [-8, 2, 25, 5, 'family'],\n","     'BEAUTY': [-8, 7, 25, 5, 'other'],\n","     'BEVERAGES': [0, 0, 40, 40, 'food'],\n","     'BOOKS': [0, 0, 55, 15, 'other'],\n","     'BREAD/BAKERY': [-3, 0, 30, 30, 'food'],\n","     'CELEBRATION': [-5, 5, 50, 20, 'family'],\n","     'CLEANING': [-8, 3, 40, 20, 'food'],\n","     'DAIRY': [-4, 0, 40, 40, 'food'],\n","     'DELI': [3, 6, 40, 20, 'food'],\n","     'EGGS': [-4, -5, 40, 20, 'food'],\n","     'FROZEN FOODS': [-4, -3, 40, 20, 'food'],\n","     'GROCERY I': [-4, 3, 40, 20, 'food'],\n","     'GROCERY II': [-4, 3, 40, 20, 'food'],\n","     'HARDWARE': [10, 10, 30, 20, 'other'],\n","     'HOME AND KITCHEN I': [-10, 4, 40, 20, 'family'],\n","     'HOME AND KITCHEN II': [-10, 4, 40, 20, 'family'],\n","     'HOME APPLIANCES': [0, 4, 40, 20, 'family'],\n","     'HOME CARE': [-10, 4, 40, 20, 'family'],\n","     'LADIESWEAR': [-10, 4, 40, 20, 'other'],\n","     'LAWN AND GARDEN': [-10, 4, 40, 20, 'family'],\n","     'LINGERIE': [-10, 4, 40, 2, 'other'],\n","     'LIQUOR,WINE,BEER': [4, 8, 40, 20, 'food'],\n","     'MAGAZINES': [-6, -7, 50, 20, 'other'],\n","     'MEATS': [-4, 5, 40, 20, 'food'],\n","     'PERSONAL CARE': [-5, 5, 40, 20, 'family'],\n","     'PET SUPPLIES': [-5, 0, 40, 20, 'family'],\n","     'PLAYERS AND ELECTRONICS': [5, 5, 25, 10, 'other'],\n","     'POULTRY': [-7, -4, 40, 20, 'food'],\n","     'PREPARED FOODS': [0, 6, 30, 10, 'food'],\n","     'PRODUCE': [0, 0, 40, 40, 'other'],\n","     'SCHOOL AND OFFICE SUPPLIES': [3, 3, 25, 15, 'family'],\n","     'SEAFOOD': [-5, 8, 40, 20, 'food']\n","    }\n","    \n","    sex_dict = {}\n","    luxury_dict = {}\n","    age_mean_dict = {}\n","    age_var_dict = {}\n","    type_dict = {}\n","    for i in tags.keys():\n","        sex_dict[i] = tags[i][0]\n","        luxury_dict[i] = tags[i][1]\n","        age_mean_dict[i] = tags[i][2]\n","        age_var_dict[i] = tags[i][3]\n","        type_dict[i] = tags[i][4]\n","    return [sex_dict, luxury_dict, age_mean_dict, age_var_dict, type_dict]"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:20.033174Z","iopub.status.busy":"2022-12-12T23:57:20.032726Z","iopub.status.idle":"2022-12-12T23:57:20.054010Z","shell.execute_reply":"2022-12-12T23:57:20.052887Z","shell.execute_reply.started":"2022-12-12T23:57:20.033142Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    The code creates a function called get_oil_dict() that takes in a DataFrame called oil as an input. \n","    The function estimates the prices of gaps (times when the market was not open) in the oil prices data, \n","    and then creates a dictionary where the keys are the day numbers from the start of 2013 \n","    (as indicated by the days_from_2013 column in the oil DataFrame) and the values are the estimated oil prices for each day. \n","    \n","    The function then returns this dictionary.\n","\"\"\"\n","\n","def get_oil_dict(oil):\n","    # estimate price of gaps (market don't work on weekends and holidays)\n","    price_estim = [-1] * (oil['days_from_2013'][oil.shape[0] - 1] + 1)\n","    price_estim[0] = 93.14\n","    for i in range(1, oil.shape[0]):\n","        price_estim[oil['days_from_2013'][i]] = oil['dcoilwtico'][i]\n","\n","    for i in range (len(price_estim)):\n","        if price_estim[i] == -1 or math.isnan(price_estim[i]):\n","            tj = -1\n","            for j in range(i + 1, len(price_estim)):\n","                if price_estim[j] != -1 and (not math.isnan(price_estim[j])):\n","                    tj = j\n","                    break\n","\n","            for j in range(i, tj):\n","                price_estim[j] = ((tj - j) * price_estim[i - 1] + (j - i) * price_estim[tj]) / (tj - i)\n","\n","            i = tj\n","\n","    oil_dict = dict(zip(np.arange(len(price_estim)), price_estim))\n","    return oil_dict"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Time Preprocessing columns"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:20.056175Z","iopub.status.busy":"2022-12-12T23:57:20.055276Z","iopub.status.idle":"2022-12-12T23:57:20.075518Z","shell.execute_reply":"2022-12-12T23:57:20.074676Z","shell.execute_reply.started":"2022-12-12T23:57:20.056137Z"},"papermill":{"duration":0.045556,"end_time":"2022-05-27T10:46:05.592039","exception":false,"start_time":"2022-05-27T10:46:05.546483","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"\n","    The add_features function is used to add new features to the original dataframe. \n","    The new features include the year, quarter, month, day, day of week, week of year, and whether the day is a weekend or not. \n","    The function also adds one-hot encoded features for the year, quarter, day of week, store, event type, and state. \n","    It also adds features generated by the DeterministicProcess class. \n","    Finally, the function adds a new feature called outliers which indicates whether a sales value is an outlier (defined as a value greater than 30,000). \n","    The function then drops the daysinmonth, month, and city columns from the dataframe.\n","\"\"\"\n","\n","def add_features (orig_df):\n","    \n","    # Make a copy of the original DataFrame\n","    df = orig_df.copy()\n","    \n","    # Add new features\n","    df['year'] = df.index.year.astype('int')\n","    df['quarter'] = df.index.quarter.astype('int')\n","    df['month'] = df.index.month.astype('int')\n","    df['day'] = df.index.day.astype('int')\n","    df['dayofweek'] = df.index.day_of_week.astype('int')\n","    df['weekofyear'] = df.index.week.astype('int')\n","    df['isweekend'] = df.dayofweek.apply(lambda x: 1 if x in (5,6) else 0)\n","    df['startschool'] = df.month.apply(lambda x: 1 if x in (4,5,8,9) else 0)\n","    \n","    df['daysinmonth'] = df.index.days_in_month.astype('int')\n","    \n","    # Add one-hot encoded features\n","    df = pd.get_dummies(df, columns=['year'], drop_first=True)\n","    df = pd.get_dummies(df, columns=['quarter'], drop_first=True)\n","    df = pd.get_dummies(df, columns=['dayofweek'], drop_first=True)\n","    df = pd.get_dummies(df, columns=['store'], drop_first=True)\n","    df = pd.get_dummies(df, columns=['event_type'], drop_first=True)\n","    df = pd.get_dummies(df, columns=['isevent'], drop_first=True)\n","    df = pd.get_dummies(df, columns=['state'], drop_first=True)\n","    \n","    # Add DeterministicProcess features\n","    fourierA = CalendarFourier(freq='A', order=5)\n","    fourierM = CalendarFourier(freq='M', order=2)\n","    fourierW = CalendarFourier(freq='W', order=4)\n","\n","    dp = DeterministicProcess(index=df.index,\n","                          order=1,\n","                          seasonal=False,\n","                          constant=False,\n","                          additional_terms=[fourierA, fourierM, fourierW],\n","                          drop=True)\n","    dp_df = dp.in_sample()\n","    df = pd.concat([df, dp_df], axis=1)\n","    \n","    # Add outliers\n","    df['outliers'] = df.sales.apply(lambda x: 1 if x>30000 else 0)\n","    \n","    # Remove unnecessary features\n","    df.drop(columns=['daysinmonth', 'month', 'city'], inplace=True)\n","    \n","    return df"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:20.078195Z","iopub.status.busy":"2022-12-12T23:57:20.076889Z","iopub.status.idle":"2022-12-12T23:57:20.105069Z","shell.execute_reply":"2022-12-12T23:57:20.103580Z","shell.execute_reply.started":"2022-12-12T23:57:20.078142Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    The add_my function is used to add new features to the original dataframe.\n","\"\"\"\n","import math\n","\n","def add_my(df):\n","    # read\n","    train = pd.read_csv('../input/store-sales-time-series-forecasting/train.csv')\n","    oil = pd.read_csv('../input/store-sales-time-series-forecasting/oil.csv')\n","    trans = pd.read_csv('../input/store-sales-time-series-forecasting/transactions.csv')\n","    \n","    # add 'days_from_2013' for easy shifting\n","    df['days_from_2013'] = (pd.to_datetime(df.index.get_level_values(2)) - pd.to_datetime('2013-01-01')).days\n","    train['days_from_2013'] = (pd.to_datetime(train['date']) - pd.to_datetime('2013-01-01')).dt.days\n","    oil['days_from_2013'] = (pd.to_datetime(oil['date']) - pd.to_datetime('2013-01-01')).dt.days\n","    trans['days_from_2013'] = (pd.to_datetime(trans['date']) - pd.to_datetime('2013-01-01')).dt.days\n","    \n","    # groupby features\n","    gr_day = train.groupby('days_from_2013')['sales'].mean()\n","    gr_store = train.groupby('store_nbr')['sales'].mean()\n","    gr_family = train.groupby('family')['sales'].mean()\n","\n","    days = [16, 18, 20, 21, 25, 28, 30, 35, 42, 60, 90, 120, 180, 365]\n","    for i in days:\n","        df['days_' + str(i)] = df['days_from_2013'] - i\n","        df['days_lagged' + str(i)] = df['days_' + str(i)].map(gr_day).fillna(0)\n","        df = df.drop(columns=['days_' + str(i)])\n","\n","    df['store_gb'] = df.index.get_level_values(0).map(gr_store)\n","    df['family_gb'] = df.index.get_level_values(1).map(gr_family)\n","    \n","    oil_dict = get_oil_dict(oil)\n","\n","    # lagged oil\n","    days = [0, 1, 2, 3, 4, 5, 6, 7, 10, 14, 21, 30, 60, 90, 120, 180, 360]\n","    for i in days:\n","        df['days_' + str(i)] = df['days_from_2013'] - i\n","        df['oil_lagged' + str(i)] = df['days_' + str(i)].map(oil_dict)\n","        df = df.drop(columns=['days_' + str(i)])\n","        \n","    # lagged transactions\n","    # # fill trans dict\n","    trans_dict = {}\n","    for ii in range(trans.shape[0]):\n","        i = trans.loc[ii]\n","        trans_dict[tuple([i['store_nbr'], i['days_from_2013']])] = i['transactions']\n","    \n","    def transaction_get_value(a, b):\n","        try:\n","            return trans_dict[tuple([a, (pd.to_datetime(b) - pd.to_datetime('2013-01-01').dt.days)])]\n","        except:\n","            return 0\n","    # Create a list with the number of days that need to be lagged\n","    # For each of the days, create a new column with the number of days lagged and another column with the oil price\n","    # lagged to the number of days\n","    days = [16, 18, 20, 21, 25, 28, 30, 35, 42, 60, 90, 120, 180, 365]\n","    for i in days:\n","        df['days_' + str(i)] = df['days_from_2013'] - i\n","        df['oil_lagged' + str(i)] = df['days_' + str(i)].map(oil_dict)\n","        df['trans_lagged' + str(i)] = [transaction_get_value(*a) for a in tuple(zip(df.index.get_level_values(0),\n","                                                                        df.index.get_level_values(2)))]\n","        df = df.drop(columns=['days_' + str(i)])\n","\n","    sex_dict, luxury_dict, age_mean_dict, age_var_dict, type_dict = tags_to_dict()\n","    df['tag_sex'] = df.index.get_level_values(1).map(sex_dict)\n","    df['tag_luxury'] = df.index.get_level_values(1).map(luxury_dict)\n","    df['tag_age_mean'] = df.index.get_level_values(1).map(age_mean_dict)\n","    df['tag_age_var'] = df.index.get_level_values(1).map(age_var_dict)\n","    df['tag_type'] = df.index.get_level_values(1).map(type_dict)\n","    df = pd.get_dummies(df, columns=['tag_type'])\n","    \n","    df['tag_age_min'] = df['tag_age_mean'] - df['tag_age_var']\n","    df['tag_age_max'] = df['tag_age_mean'] + df['tag_age_var']\n","    return df"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-12-12T23:57:20.108120Z","iopub.status.busy":"2022-12-12T23:57:20.107267Z","iopub.status.idle":"2022-12-13T00:17:17.433745Z","shell.execute_reply":"2022-12-13T00:17:17.432179Z","shell.execute_reply.started":"2022-12-12T23:57:20.108077Z"},"papermill":{"duration":0.082648,"end_time":"2022-05-27T10:46:05.700498","exception":false,"start_time":"2022-05-27T10:46:05.61785","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>id</th>\n","      <th>sales</th>\n","      <th>onpromotion</th>\n","      <th>cluster</th>\n","      <th>uniquestore</th>\n","      <th>newstore</th>\n","      <th>firstday</th>\n","      <th>isclosed</th>\n","      <th>dcoilwtico</th>\n","      <th>lagoil_1_dcoilwtico</th>\n","      <th>...</th>\n","      <th>trans_lagged365</th>\n","      <th>tag_sex</th>\n","      <th>tag_luxury</th>\n","      <th>tag_age_mean</th>\n","      <th>tag_age_var</th>\n","      <th>tag_type_family</th>\n","      <th>tag_type_food</th>\n","      <th>tag_type_other</th>\n","      <th>tag_age_min</th>\n","      <th>tag_age_max</th>\n","    </tr>\n","    <tr>\n","      <th>store_nbr</th>\n","      <th>family</th>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th rowspan=\"5\" valign=\"top\">AUTOMOTIVE</th>\n","      <th>2016-06-01</th>\n","      <td>2216808</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>49.070000</td>\n","      <td>49.100000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2016-06-02</th>\n","      <td>2218590</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>49.140000</td>\n","      <td>49.070000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2016-06-03</th>\n","      <td>2220372</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>48.690000</td>\n","      <td>49.140000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2016-06-04</th>\n","      <td>2222154</td>\n","      <td>9.0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>49.030000</td>\n","      <td>48.690000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2016-06-05</th>\n","      <td>2223936</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>49.370000</td>\n","      <td>49.030000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>30</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">54</th>\n","      <th rowspan=\"5\" valign=\"top\">SEAFOOD</th>\n","      <th>2017-08-27</th>\n","      <td>3022139</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>46.816667</td>\n","      <td>47.233333</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>-5</td>\n","      <td>8</td>\n","      <td>40</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2017-08-28</th>\n","      <td>3023921</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>46.400000</td>\n","      <td>46.816667</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>-5</td>\n","      <td>8</td>\n","      <td>40</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2017-08-29</th>\n","      <td>3025703</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>46.460000</td>\n","      <td>46.400000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>-5</td>\n","      <td>8</td>\n","      <td>40</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2017-08-30</th>\n","      <td>3027485</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>45.960000</td>\n","      <td>46.460000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>-5</td>\n","      <td>8</td>\n","      <td>40</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2017-08-31</th>\n","      <td>3029267</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>47.260000</td>\n","      <td>45.960000</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>-5</td>\n","      <td>8</td>\n","      <td>40</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>20</td>\n","      <td>60</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>812592 rows  160 columns</p>\n","</div>"],"text/plain":["                                      id  sales  onpromotion  cluster  \\\n","store_nbr family     date                                               \n","1         AUTOMOTIVE 2016-06-01  2216808    3.0            0       13   \n","                     2016-06-02  2218590    1.0            0       13   \n","                     2016-06-03  2220372    4.0            0       13   \n","                     2016-06-04  2222154    9.0            0       13   \n","                     2016-06-05  2223936    2.0            0       13   \n","...                                  ...    ...          ...      ...   \n","54        SEAFOOD    2017-08-27  3022139    NaN            0        3   \n","                     2017-08-28  3023921    NaN            0        3   \n","                     2017-08-29  3025703    NaN            0        3   \n","                     2017-08-30  3027485    NaN            0        3   \n","                     2017-08-31  3029267    NaN            0        3   \n","\n","                                 uniquestore  newstore  firstday  isclosed  \\\n","store_nbr family     date                                                    \n","1         AUTOMOTIVE 2016-06-01            0         0         0         0   \n","                     2016-06-02            0         0         0         0   \n","                     2016-06-03            0         0         0         0   \n","                     2016-06-04            0         0         0         0   \n","                     2016-06-05            0         0         0         0   \n","...                                      ...       ...       ...       ...   \n","54        SEAFOOD    2017-08-27            1         0         0         0   \n","                     2017-08-28            1         0         0         0   \n","                     2017-08-29            1         0         0         0   \n","                     2017-08-30            1         0         0         0   \n","                     2017-08-31            1         0         0         0   \n","\n","                                 dcoilwtico  lagoil_1_dcoilwtico  ...  \\\n","store_nbr family     date                                         ...   \n","1         AUTOMOTIVE 2016-06-01   49.070000            49.100000  ...   \n","                     2016-06-02   49.140000            49.070000  ...   \n","                     2016-06-03   48.690000            49.140000  ...   \n","                     2016-06-04   49.030000            48.690000  ...   \n","                     2016-06-05   49.370000            49.030000  ...   \n","...                                     ...                  ...  ...   \n","54        SEAFOOD    2017-08-27   46.816667            47.233333  ...   \n","                     2017-08-28   46.400000            46.816667  ...   \n","                     2017-08-29   46.460000            46.400000  ...   \n","                     2017-08-30   45.960000            46.460000  ...   \n","                     2017-08-31   47.260000            45.960000  ...   \n","\n","                                 trans_lagged365  tag_sex  tag_luxury  \\\n","store_nbr family     date                                               \n","1         AUTOMOTIVE 2016-06-01                0        4           7   \n","                     2016-06-02                0        4           7   \n","                     2016-06-03                0        4           7   \n","                     2016-06-04                0        4           7   \n","                     2016-06-05                0        4           7   \n","...                                          ...      ...         ...   \n","54        SEAFOOD    2017-08-27                0       -5           8   \n","                     2017-08-28                0       -5           8   \n","                     2017-08-29                0       -5           8   \n","                     2017-08-30                0       -5           8   \n","                     2017-08-31                0       -5           8   \n","\n","                                 tag_age_mean  tag_age_var  tag_type_family  \\\n","store_nbr family     date                                                     \n","1         AUTOMOTIVE 2016-06-01            30           10                1   \n","                     2016-06-02            30           10                1   \n","                     2016-06-03            30           10                1   \n","                     2016-06-04            30           10                1   \n","                     2016-06-05            30           10                1   \n","...                                       ...          ...              ...   \n","54        SEAFOOD    2017-08-27            40           20                0   \n","                     2017-08-28            40           20                0   \n","                     2017-08-29            40           20                0   \n","                     2017-08-30            40           20                0   \n","                     2017-08-31            40           20                0   \n","\n","                                 tag_type_food  tag_type_other  tag_age_min  \\\n","store_nbr family     date                                                     \n","1         AUTOMOTIVE 2016-06-01              0               0           20   \n","                     2016-06-02              0               0           20   \n","                     2016-06-03              0               0           20   \n","                     2016-06-04              0               0           20   \n","                     2016-06-05              0               0           20   \n","...                                        ...             ...          ...   \n","54        SEAFOOD    2017-08-27              1               0           20   \n","                     2017-08-28              1               0           20   \n","                     2017-08-29              1               0           20   \n","                     2017-08-30              1               0           20   \n","                     2017-08-31              1               0           20   \n","\n","                                 tag_age_max  \n","store_nbr family     date                     \n","1         AUTOMOTIVE 2016-06-01           40  \n","                     2016-06-02           40  \n","                     2016-06-03           40  \n","                     2016-06-04           40  \n","                     2016-06-05           40  \n","...                                      ...  \n","54        SEAFOOD    2017-08-27           60  \n","                     2017-08-28           60  \n","                     2017-08-29           60  \n","                     2017-08-30           60  \n","                     2017-08-31           60  \n","\n","[812592 rows x 160 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df = add_features(final_df).loc[:date['date_end_test'],:].reset_index().set_index(['store_nbr', 'family', 'date']).sort_index()\n","\n","# Fill in missing data with zeros\n","df['16_tra'] = df['16_tra'].fillna(0)\n","df['21_tra'] = df['21_tra'].fillna(0)\n","df['30_tra'] = df['30_tra'].fillna(0)\n","df['60_tra'] = df['60_tra'].fillna(0)\n","\n","# Add additional features using the `add_my` function\n","df = add_my(df)\n","\n","# Display the resulting DataFrame\n","display(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Sklearn Random Forest Regression"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T00:17:17.437949Z","iopub.status.busy":"2022-12-13T00:17:17.436724Z","iopub.status.idle":"2022-12-13T00:17:17.446192Z","shell.execute_reply":"2022-12-13T00:17:17.445073Z","shell.execute_reply.started":"2022-12-13T00:17:17.437881Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","# Linear Regression model\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","# Random Forest and Gradient Boosting models\n","from sklearn.metrics import mean_squared_log_error as msle\n","# Metric for regression problems\n","from tqdm import tqdm\n","# Progress bar\n","\n","from xgboost import XGBRegressor\n","# XGBoost model\n","from lightgbm import LGBMRegressor\n","# LightGBM model\n","from catboost import Pool, CatBoostRegressor\n","# CatBoost model\n","\n","import optuna\n","# Optuna for hyperparameter tuning"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T00:17:17.448500Z","iopub.status.busy":"2022-12-13T00:17:17.447890Z","iopub.status.idle":"2022-12-13T00:17:17.469201Z","shell.execute_reply":"2022-12-13T00:17:17.467580Z","shell.execute_reply.started":"2022-12-13T00:17:17.448461Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","This function is used to optimize the hyperparameters of the model\n","\"\"\"\n","\n","def objective(trial):\n","    # Define parameters\n","    params = {\n","        'criterion': 'squared_error',\n","        'bootstrap': trial.suggest_categorical('bootstrap',['True','False']),\n","        'max_depth': trial.suggest_int('max_depth', 1, 10000),\n","        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt','log2']),\n","        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 1, 10000),\n","        'n_estimators': trial.suggest_int('n_estimators', 30, 5000),\n","        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n","        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50)\n","    }\n","    weights_distribution = trial.suggest_categorical('weights_distribution', [1, 2, 3, 4, 5])\n","    \n","    # Define variables to store the result\n","    sm_rf = 0\n","    cnt = 0\n","    \n","    # Iterate through each store_id\n","    for i in df.index.get_level_values(0).unique():\n","        # Iterate through each dept_id\n","        for j in df.index.get_level_values(1).unique():\n","            # Select rows for a store_id-dept_id pair\n","            df_ = df.loc[(i, j)]\n","            # Drop 'id', 'transactions' columns\n","            df_ = df_.drop(columns=['id', 'transactions'])\n","\n","            # Split train and test\n","            train = df_[~df_['sales'].isna()]\n","            X_test = df_[df_['sales'].isna()].drop(columns=['sales'])\n","            X_train, X_val, y_train, y_val = my_split_func(train)\n","\n","            # Apply log transformation\n","            y_train = np.log1p(y_train)\n","            y_val = np.log1p(y_val)\n","            \n","            # Create model\n","            model = RandomForestRegressor(**params)\n","            # Define weights\n","            weights = get_weights_distribution(weights_distribution, X_train.index)\n","            # Fit the model\n","            model.fit(X_train, y_train, sample_weight=weights)\n","            # Make predictions\n","            preds = model.predict(X_val)\n","            \n","            # Calculate MSLE\n","            sm_rf += msle(np.exp(y_val) - 1, (np.exp(preds) - 1).clip(0))\n","            cnt += 1\n","\n","    # Calculate RMSLE\n","    logs.append([(sm_rf / cnt)**0.5, params, weights_distribution])\n","    return (sm_rf / cnt)**0.5"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T00:17:17.472095Z","iopub.status.busy":"2022-12-13T00:17:17.471488Z","iopub.status.idle":"2022-12-13T00:17:17.495804Z","shell.execute_reply":"2022-12-13T00:17:17.494002Z","shell.execute_reply.started":"2022-12-13T00:17:17.472050Z"},"trusted":true},"outputs":[],"source":["# # 1. Create a study to track each trial.\n","# study = optuna.create_study(direction='minimize')\n","\n","# # 2. Run the optimization task.\n","# study.optimize(objective, n_trials=1000)"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T00:17:17.499200Z","iopub.status.busy":"2022-12-13T00:17:17.498547Z","iopub.status.idle":"2022-12-13T00:17:17.540469Z","shell.execute_reply":"2022-12-13T00:17:17.537496Z","shell.execute_reply.started":"2022-12-13T00:17:17.499160Z"},"trusted":true},"outputs":[],"source":["# Read the sample submission file\n","\n","ss = pd.read_csv('../input/store-sales-time-series-forecasting/sample_submission.csv')"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T00:17:17.558706Z","iopub.status.busy":"2022-12-13T00:17:17.557032Z","iopub.status.idle":"2022-12-13T00:17:17.576431Z","shell.execute_reply":"2022-12-13T00:17:17.575177Z","shell.execute_reply.started":"2022-12-13T00:17:17.558609Z"},"trusted":true},"outputs":[],"source":["# Best parameters from the trails.\n","good_p = [{'criterion': 'squared_error',\n","  'bootstrap': 'False',\n","  'max_depth': 9733,\n","  'max_features': 'auto',\n","  'max_leaf_nodes': 4730,\n","  'n_estimators': 700,\n","  'min_samples_split': 3,\n","  'min_samples_leaf': 8},] "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T00:17:17.579044Z","iopub.status.busy":"2022-12-13T00:17:17.578377Z","iopub.status.idle":"2022-12-13T03:27:21.000209Z","shell.execute_reply":"2022-12-13T03:27:20.998627Z","shell.execute_reply.started":"2022-12-13T00:17:17.578997Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 54/54 [3:10:03<00:00, 211.17s/it]  \n"]}],"source":["\"\"\"\n","    Creating a dictionary named fin_pred which will store the predictions for the test data. \n","    Then, for each unique store and day, you are selecting the rows for that store and day, dropping the 'id' \n","    and 'transactions' columns, splitting the data into train and validation sets, \n","    fitting a random forest regressor on the train data and making predictions on the test data. \n","    These predictions are then added to the fin_pred dictionary.\n","\"\"\"\n","\n","# Set the random state for reproducibility\n","random_state = 0\n","\n","# Initialize the counters\n","sm_rf = 0\n","cnt = 0\n","\n","# Create a dictionary to store the predictions\n","fin_pred = {}\n","\n","# Loop over the unique stores and days\n","for i in tqdm(df.index.get_level_values(0).unique()):\n","    for j in df.index.get_level_values(1).unique():\n","        \n","        # Get the data\n","        df_ = df.loc[(i, j)]\n","        \n","        # Get the ids\n","        test_id = df_[df_['sales'].isna()]['id']\n","        \n","        # Drop the id and transaction columns\n","        df_ = df_.drop(columns=['id', 'transactions'])\n","\n","        # Get the train data\n","        train = df_[~df_['sales'].isna()]\n","        \n","        # Get the test data\n","        X_test = df_[df_['sales'].isna()].drop(columns=['sales'])\n","        \n","        # Split the data into train and validation sets\n","        X_train, X_val, y_train, y_val = my_split_func(train)\n","\n","        # Log transform the sales\n","        y_train = np.log1p(y_train)\n","        # y_val = np.log1p(y_val)\n","\n","        # Initialize the model\n","        model = RandomForestRegressor(**good_p[0], random_state=random_state)\n","        \n","        # Get the weights\n","        weights = get_weights_distribution(5, X_train.index)\n","        \n","        # Fit the model\n","        model.fit(X_train, y_train, sample_weight=weights)\n","\n","        # Get the predictions\n","        # preds = model.predict(X_val)\n","        preds_ = model.predict(X_test)\n","\n","        # Loop over the predictions\n","        for q in range(preds_.shape[0]):\n","            fin_pred[test_id[q]] = preds_[q]\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T03:27:21.004676Z","iopub.status.busy":"2022-12-13T03:27:21.002834Z","iopub.status.idle":"2022-12-13T03:27:21.031995Z","shell.execute_reply":"2022-12-13T03:27:21.031148Z","shell.execute_reply.started":"2022-12-13T03:27:21.004587Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    This takes in a dictionary of sales predictions with store IDs as keys and\n","    sales predictions as values, and a dataframe of store IDs and their geographic\n","    locations, and returns a map of sales predictions by store locations.\n","\"\"\"\n","\n","ss['sales'] = ss['id'].map(fin_pred)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T03:27:21.034702Z","iopub.status.busy":"2022-12-13T03:27:21.033898Z","iopub.status.idle":"2022-12-13T03:27:21.046591Z","shell.execute_reply":"2022-12-13T03:27:21.045412Z","shell.execute_reply.started":"2022-12-13T03:27:21.034656Z"},"trusted":true},"outputs":[],"source":["\"\"\"applying the exponential transformation to the sales column.\"\"\"\n","\n","ss['sales'] = np.exp(ss['sales']) - 1"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T03:27:21.049253Z","iopub.status.busy":"2022-12-13T03:27:21.048745Z","iopub.status.idle":"2022-12-13T03:27:21.135949Z","shell.execute_reply":"2022-12-13T03:27:21.134844Z","shell.execute_reply.started":"2022-12-13T03:27:21.049209Z"},"papermill":{"duration":0.106685,"end_time":"2022-05-27T10:49:03.652779","exception":false,"start_time":"2022-05-27T10:49:03.546094","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\"\"\"save the final predictions to the disk as submission.csv file.\"\"\"\n","\n","ss.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"tf1","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"5245c9e18bdde30837349597c8773ef8a0f14db90e620f25eaaa03a7ba1141c0"}}},"nbformat":4,"nbformat_minor":4}
